import streamlit as st
import pdfplumber
import pandas as pd
import io
import re
from datetime import datetime

# --- 1. å®šç¾©æ¬„ä½èˆ‡é—œéµå­— ---

# æŠ“æ•¸å€¼çš„é …ç›®
SIMPLE_KEYWORDS = {
    "Pb": ["Lead", "é‰›", "Pb"],
    "Cd": ["Cadmium", "é˜", "Cd"],
    "Hg": ["Mercury", "æ±", "Hg"],
    "Cr6+": ["Hexavalent Chromium", "å…­åƒ¹é‰»", "Cr(VI)", "Chromium VI"],
    "DEHP": ["DEHP", "Di(2-ethylhexyl) phthalate", "Bis(2-ethylhexyl) phthalate"],
    "BBP": ["BBP", "Butyl benzyl phthalate"],
    "DBP": ["DBP", "Dibutyl phthalate"],
    "DIBP": ["DIBP", "Diisobutyl phthalate"],
    "PFOS": ["Perfluorooctane sulfonates", "å…¨æ°Ÿè¾›çƒ·ç£ºé…¸", "Perfluorooctane sulfonate"], 
    "F": ["Fluorine", "æ°Ÿ"],
    "CL": ["Chlorine", "æ°¯"],
    "BR": ["Bromine", "æº´"],
    "I": ["Iodine", "ç¢˜"]
}

# æŠ“ç¾¤çµ„æœ€å¤§å€¼çš„é …ç›®
GROUP_KEYWORDS = {
    "PBB": [
        "Polybrominated Biphenyls", "PBBs", "Sum of PBBs", "å¤šæº´è¯è‹¯ç¸½å’Œ",
        "Monobromobiphenyl", "Dibromobiphenyl", "Tribromobiphenyl", 
        "Tetrabromobiphenyl", "Pentabromobiphenyl", "Hexabromobiphenyl", 
        "Heptabromobiphenyl", "Octabromobiphenyl", "Nonabromobiphenyl", 
        "Decabromobiphenyl", 
        "Monobrominated", "Dibrominated", "Tribrominated", 
        "Tetrabrominated", "Pentabrominated", "Hexabrominated", 
        "Heptabrominated", "Octabrominated", "Nonabrominated", 
        "Decabrominated",
        "bromobiphenyl"
    ],
    "PBDE": [
        "Polybrominated Diphenyl Ethers", "PBDEs", "Sum of PBDEs", "å¤šæº´è¯è‹¯é†šç¸½å’Œ",
        "Monobromodiphenyl ether", "Dibromodiphenyl ether", "Tribromodiphenyl ether",
        "Tetrabromodiphenyl ether", "Pentabromodiphenyl ether", "Hexabromodiphenyl ether",
        "Heptabromodiphenyl ether", "Octabromodiphenyl ether", "Nonabromodiphenyl ether",
        "Decabromodiphenyl ether", 
        "Monobrominated Diphenyl", "Dibrominated Diphenyl", "Tribrominated Diphenyl",
        "Tetrabrominated Diphenyl", "Pentabrominated Diphenyl", "Hexabrominated Diphenyl",
        "Heptabrominated Diphenyl", "Octabrominated Diphenyl", "Nonabrominated Diphenyl",
        "Decabrominated Diphenyl",
        "bromodiphenyl ether"
    ]
}

# PFAS æ‘˜è¦æª¢æŸ¥é—œéµå­—
PFAS_SUMMARY_KEYWORDS = [
    "Per- and Polyfluoroalkyl Substances",
    "PFAS",
    "å…¨æ°Ÿ/å¤šæ°Ÿçƒ·åŸºç‰©è³ª",
    "Perfluorooctanoic acid (PFOA) and its salts", 
    "å…¨æ°ŸåŒ–åˆç‰©",
    "Perfluoro"
]

OUTPUT_COLUMNS = [
    "Pb", "Cd", "Hg", "Cr6+", "PBB", "PBDE", 
    "DEHP", "BBP", "DBP", "DIBP", 
    "PFOS", "PFAS", "F", "CL", "BR", "I", 
    "æ—¥æœŸ", "æª”æ¡ˆåç¨±"
]

# --- 2. è¼”åŠ©åŠŸèƒ½ ---

def clean_text(text):
    if not text: return ""
    return str(text).replace('\n', ' ').strip()

def extract_date_from_text(text):
    text = clean_text(text)
    patterns = [
        r"(20\d{2})[/\.-](0?[1-9]|1[0-2])[/\.-](0?[1-9]|[12][0-9]|3[01])", 
        r"(0?[1-9]|[12][0-9]|3[01])\s*[-/]\s*([a-zA-Z]{3})\s*[-/]\s*(20\d{2})", 
        r"([a-zA-Z]{3})\.?\s+(0?[1-9]|[12][0-9]|3[01])[,\s]+\s*(20\d{2})" 
    ]
    
    found_dates = []
    for pattern in patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            try:
                dt = None
                full_match = match.group(0)
                clean_str = full_match.replace(".", " ").replace(",", " ").replace("-", " ").replace("/", " ")
                clean_str = " ".join(clean_str.split())
                for fmt in ["%Y %m %d", "%d %b %Y", "%b %d %Y"]:
                    try:
                        dt = datetime.strptime(clean_str, fmt)
                        break
                    except: continue
                if dt and 2000 <= dt.year <= 2030: 
                    found_dates.append(dt)
            except: continue
    if found_dates: return max(found_dates)
    return None

def is_suspicious_limit_value(val):
    try:
        n = float(val)
        if n in [1000.0, 100.0, 50.0]: return True
        return False
    except: return False

def parse_value_priority(value_str):
    raw_val = clean_text(value_str)
    if "(" in raw_val: raw_val = raw_val.split("(")[0].strip()
    val = raw_val.replace("mg/kg", "").replace("ppm", "").replace("%", "").replace("Âµg/cmÂ²", "").strip()
    
    if not val: return (0, 0, "")
    val_lower = val.lower()

    # æ’é™¤æ¸…å–® (æ³¨æ„ï¼šé€™è£¡ä¸æ’é™¤ "limit" å­—çœ¼ï¼Œå› ç‚º SGS çš„ limit æ˜¯æ¨™é¡Œï¼Œä¸æ˜¯æ•¸å€¼)
    if val_lower in ["result", "limit", "mdl", "loq", "rl", "unit", "method", "004", "001", "no.1", "---", "-", "limits"]: 
        return (0, 0, "")

    if re.search(r"\d+-\d+-\d+", val): return (0, 0, "") 
    if is_suspicious_limit_value(val): return (0, 0, "") 

    if "nd" in val_lower or "n.d." in val_lower or "<" in val_lower: return (1, 0, "N.D.")
    if "negative" in val_lower or "é™°æ€§" in val_lower: return (2, 0, "NEGATIVE")
    
    num_match = re.search(r"([\d\.]+)", val)
    if num_match:
        try:
            number = float(num_match.group(1))
            return (3, number, num_match.group(1))
        except: pass
            
    return (0, 0, val)

# --- 3. æ ¸å¿ƒï¼šæ™ºæ…§æ¬„ä½è­˜åˆ¥ ---

def check_pfas_in_summary(text):
    txt_lower = text.lower()
    for kw in PFAS_SUMMARY_KEYWORDS:
        if kw.lower() in txt_lower: return True
    return False

def identify_columns(table):
    """
    v23.0: ç²¾æº–å€åˆ†ã€ŒSGS ä¸»è¡¨æ ¼ã€èˆ‡ã€Œé™å€¼/åƒè€ƒè¡¨æ ¼ã€
    """
    item_idx = -1
    result_idx = -1
    
    max_scan_rows = min(3, len(table))
    full_header_text = ""
    for r in range(max_scan_rows):
        full_header_text += " ".join([str(c).lower() for c in table[r] if c]) + " "
    
    # 1. è­˜åˆ¥çµæœæ¬„ä½ (é—œéµæ­¥é©Ÿ)
    # å°‹æ‰¾ 001~009, Result, No.1, Green material
    # SGS çš„é—œéµæ˜¯ "001", "002"
    for r_idx in range(max_scan_rows):
        row = table[r_idx]
        for c_idx, cell in enumerate(row):
            txt = clean_text(cell).lower()
            if not txt: continue
            
            if "test item" in txt or "tested item" in txt or "æ¸¬è©¦é …ç›®" in txt:
                if item_idx == -1: item_idx = c_idx
            
            # çµæœæ¬„ä½é—œéµå­—
            if ("result" in txt or "çµæœ" in txt or re.search(r"00[1-9]", txt) or 
                "no." in txt or "green" in txt or "submitted" in txt or "composite" in txt):
                # å†æ¬¡ç¢ºèªä¸æ˜¯ CAS No
                if "cas" not in txt:
                    if result_idx == -1: result_idx = c_idx

    # 2. åˆ¤æ–·æ˜¯å¦ç‚ºã€Œè¦è·³éã€çš„è¡¨æ ¼
    # åªæœ‰ç•¶å®ƒæ˜¯ "é™ç”¨ç‰©è³ªæ¸…å–®" æˆ– "PFAS åƒè€ƒè¡¨"ï¼Œä¸”**æ‰¾ä¸åˆ°**çµæœæ¬„ä½æ™‚ï¼Œæ‰è·³é
    # SGS ä¸»è¡¨æ ¼é›–ç„¶æœ‰ "Limit"ï¼Œä½†ä¹Ÿæœ‰ "002"ï¼Œæ‰€ä»¥ result_idx æœƒè¢«æ‰¾åˆ°ï¼Œé€™è£¡å°±ä¸æœƒè·³é -> ä¿®æ­£æˆåŠŸï¼
    is_reference_table = False
    if result_idx == -1: # åªæœ‰åœ¨æ‰¾ä¸åˆ°çµæœæ¬„æ™‚ï¼Œæ‰å»æª¢æŸ¥æ˜¯å¦ç‚ºåƒè€ƒè¡¨
        if ("restricted substances" in full_header_text or "limits" in full_header_text or 
            "group name" in full_header_text or "substance name" in full_header_text):
            is_reference_table = True

    return item_idx, result_idx, is_reference_table

def process_files(files):
    data_pool = {key: [] for key in OUTPUT_COLUMNS if key not in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]}
    all_dates = []
    pb_tracker = {"max_score": -1, "max_value": -1.0, "filenames": []}
    
    progress_bar = st.progress(0)
    
    for i, file in enumerate(files):
        filename = file.name
        file_group_data = {key: [] for key in GROUP_KEYWORDS.keys()}
        
        try:
            with pdfplumber.open(file) as pdf:
                file_dates = []
                first_few_pages_text = ""
                
                # 1. æ—¥æœŸ & PFAS
                for p_idx in range(min(2, len(pdf.pages))):
                    page_txt = pdf.pages[p_idx].extract_text()
                    if page_txt:
                        first_few_pages_text += page_txt
                        d = extract_date_from_text(page_txt)
                        if d: file_dates.append(d)
                
                if file_dates: all_dates.append((max(file_dates), filename))
                
                if check_pfas_in_summary(first_few_pages_text):
                    data_pool["PFAS"].append({"priority": (4, 0, "REPORT"), "filename": filename})

                # 2. è¡¨æ ¼æƒæ
                last_result_idx = -1 
                last_item_idx = 0

                for page in pdf.pages:
                    tables = page.extract_tables()
                    for table in tables:
                        if not table or len(table) < 2: continue
                        
                        item_idx, result_idx, is_skip_table = identify_columns(table)
                        
                        if is_skip_table: continue 

                        # è¡¨é ­è¨˜æ†¶
                        if result_idx != -1:
                            last_result_idx = result_idx
                            last_item_idx = item_idx if item_idx != -1 else 0
                        else:
                            if last_result_idx != -1 and len(table[0]) > 3:
                                result_idx = last_result_idx
                                item_idx = last_item_idx
                        
                        for row_idx, row in enumerate(table):
                            clean_row = [clean_text(cell) for cell in row]
                            row_txt = "".join(clean_row).lower()
                            if "test item" in row_txt or "result" in row_txt or "restricted" in row_txt: continue
                            if not any(clean_row): continue
                            
                            target_item_col = item_idx if item_idx != -1 else 0
                            if target_item_col >= len(clean_row): continue
                            item_name = clean_row[target_item_col]
                            
                            if "pvc" in item_name.lower() or "polyvinyl" in item_name.lower(): continue

                            result = ""
                            # A. å„ªå…ˆç”¨å®šä½ (SGS æœƒèµ°é€™è£¡ï¼Œç›´æ¥æŠ“ 002 é‚£ä¸€æ¬„)
                            if result_idx != -1 and result_idx < len(clean_row):
                                result = clean_row[result_idx]
                            
                            # B. å‚™æ´ (å€’è‘—æ‰¾ï¼Œä½†æœ‰é˜²ç«ç‰†)
                            if not result:
                                for cell in reversed(clean_row):
                                    c_lower = cell.lower()
                                    if not cell: continue
                                    if "nd" in c_lower or "n.d." in c_lower or "negative" in c_lower:
                                        result = cell
                                        break
                                    if re.search(r"^\d+(\.\d+)?$", cell):
                                        if float(cell) in [1000, 100, 50]: continue # é˜²ç«ç‰†
                                        result = cell
                                        break
                            
                            priority = parse_value_priority(result)
                            if priority[0] == 0: continue 

                            # Simple
                            for target_key, keywords in SIMPLE_KEYWORDS.items():
                                for kw in keywords:
                                    if kw.lower() in item_name.lower():
                                        if target_key == "PFOS" and "related" in item_name.lower(): continue 
                                        data_pool[target_key].append({"priority": priority, "filename": filename})
                                        
                                        if target_key == "Pb":
                                            current_score = priority[0]
                                            current_val = priority[1]
                                            if current_score > pb_tracker["max_score"]:
                                                pb_tracker["max_score"] = current_score
                                                pb_tracker["max_value"] = current_val
                                                pb_tracker["filenames"] = [filename]
                                            elif current_score == 3 and current_val > pb_tracker["max_value"]:
                                                pb_tracker["max_value"] = current_val
                                                pb_tracker["filenames"] = [filename]
                                            elif current_score == 3 and current_val == pb_tracker["max_value"]:
                                                if filename not in pb_tracker["filenames"]:
                                                    pb_tracker["filenames"].append(filename)
                                        break

                            # Group
                            for group_key, keywords in GROUP_KEYWORDS.items():
                                for kw in keywords:
                                    if kw.lower() in item_name.lower():
                                        file_group_data[group_key].append(priority)
                                        break
            
            # æª”æ¡ˆçµç®— (PBB/PBDE)
            for group_key, values in file_group_data.items():
                if values:
                    best_in_file = sorted(values, key=lambda x: (x[0], x[1]), reverse=True)[0]
                    data_pool[group_key].append({
                        "priority": best_in_file,
                        "filename": filename
                    })

        except Exception as e:
            st.warning(f"æª”æ¡ˆ {filename} è§£æç•°å¸¸: {e}")
        
        progress_bar.progress((i + 1) / len(files))

    # èšåˆ
    final_row = {}
    for key in OUTPUT_COLUMNS:
        if key in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]: continue
        candidates = data_pool.get(key, [])
        if not candidates:
            final_row[key] = "" 
            continue
        best_record = sorted(candidates, key=lambda x: (x['priority'][0], x['priority'][1]), reverse=True)[0]
        final_row[key] = best_record['priority'][2]

    # æ—¥æœŸèˆ‡æª”å
    final_date_str = ""
    latest_file = ""
    if all_dates:
        latest_date_record = sorted(all_dates, key=lambda x: x[0], reverse=True)[0]
        final_date_str = latest_date_record[0].strftime("%Y/%m/%d")
        latest_file = latest_date_record[1]
    
    final_row["æ—¥æœŸ"] = final_date_str
    
    if pb_tracker["filenames"]:
        final_row["æª”æ¡ˆåç¨±"] = ", ".join(pb_tracker["filenames"])
    else:
        final_row["æª”æ¡ˆåç¨±"] = latest_file if latest_file else (files[0].name if files else "")

    return [final_row]

# --- ä»‹é¢ ---
st.set_page_config(page_title="SGS å ±å‘Šèšåˆå·¥å…· v23.0", layout="wide")
st.title("ğŸ“„ è¬ç”¨å‹æª¢æ¸¬å ±å‘Šèšåˆå·¥å…· (v23.0)")
st.info("ğŸ’¡ v23.0ï¼šä¿®æ­£ SGS ä¸»è¡¨æ ¼ç•¥éå•é¡Œï¼Œæ”¯æ´å« Limit/001 çš„æ··åˆè¡¨æ ¼ï¼Œçµæœå¤§å¯«é¡¯ç¤ºã€‚")

uploaded_files = st.file_uploader("è«‹ä¸€æ¬¡é¸å–æ‰€æœ‰ PDF æª”æ¡ˆ", type="pdf", accept_multiple_files=True)

if uploaded_files:
    if st.button("ğŸ”„ é‡æ–°åŸ·è¡Œ"): st.rerun()

    try:
        result_data = process_files(uploaded_files)
        df = pd.DataFrame(result_data)
        for col in OUTPUT_COLUMNS:
            if col not in df.columns: df[col] = ""
        df = df[OUTPUT_COLUMNS]

        st.success("âœ… è™•ç†å®Œæˆï¼")
        st.dataframe(df)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Summary')
        
        st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel", data=output.getvalue(), file_name="SGS_Summary_v23.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        
    except Exception as e:
        st.error(f"ç³»çµ±éŒ¯èª¤: {e}")
