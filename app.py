import streamlit as st
import pdfplumber
import pandas as pd
import io
import re
from datetime import datetime

# --- 1. å®šç¾©æ¬„ä½èˆ‡é—œéµå­—å°ç…§è¡¨ ---
KEYWORD_MAP = {
    "Pb": ["Lead", "é‰›", "Pb"],
    "Cd": ["Cadmium", "é˜", "Cd"],
    "Hg": ["Mercury", "æ±", "Hg"],
    "Cr6+": ["Hexavalent Chromium", "å…­åƒ¹é‰»", "Cr(VI)"],
    "PBB": ["Sum of PBBs", "å¤šæº´è¯è‹¯ç¸½å’Œ"],
    "PBDE": ["Sum of PBDEs", "å¤šæº´è¯è‹¯é†šç¸½å’Œ"],
    "DEHP": ["DEHP", "Di(2-ethylhexyl) phthalate"],
    "BBP": ["BBP", "Butyl benzyl phthalate"],
    "DBP": ["DBP", "Dibutyl phthalate"],
    "DIBP": ["DIBP", "Diisobutyl phthalate"],
    "PFOS": ["PFOS", "Perfluorooctane sulfonates"],
    "F": ["Fluorine", "æ°Ÿ"],
    "CL": ["Chlorine", "æ°¯"],
    "BR": ["Bromine", "æº´"],
    "I": ["Iodine", "ç¢˜"]
}

OUTPUT_COLUMNS = [
    "Pb", "Cd", "Hg", "Cr6+", "PBB", "PBDE", 
    "DEHP", "BBP", "DBP", "DIBP", 
    "PFOS", "F", "CL", "BR", "I", 
    "å–®ä½", "æ—¥æœŸ", "æª”æ¡ˆåç¨±"
]

# --- 2. è¼”åŠ©åŠŸèƒ½ ---

def extract_date_from_text(text):
    """
    ä¿®æ­£ç‰ˆï¼šå¢å¼·å° SGS æ—¥æœŸæ ¼å¼çš„ç›¸å®¹æ€§
    èƒ½æŠ“å–: "Date: ...", "æ—¥æœŸ: ...", "æ—¥æœŸ(Date): ..."
    """
    # Regex è§£é‡‹: (?:Date|æ—¥æœŸ) = æ‰¾é€™å…©å€‹å­—é–‹é ­, .*? = ä¸­é–“å¯æœ‰ä»»æ„é›œå­—, :\s* = å†’è™Ÿèˆ‡ç©ºç™½
    match = re.search(r"(?:Date|æ—¥æœŸ).*?:\s*([0-9]{2}-[a-zA-Z]{3}-[0-9]{4})", text, re.IGNORECASE)
    if match:
        try:
            date_str = match.group(1)
            return datetime.strptime(date_str, "%d-%b-%Y")
        except:
            return None
    return None

def parse_value_priority(value_str):
    """
    æ±ºå®šæ•¸å€¼çš„å„ªå…ˆé †åº
    ä¿®æ­£ç‰ˆï¼šå¼·åˆ¶ç§»é™¤å–®ä½å­—ä¸²
    """
    # 1. è½‰å­—ä¸²ä¸¦å¼·åˆ¶ç§»é™¤å¸¸è¦‹å–®ä½
    val = str(value_str).replace("mg/kg", "").replace("ppm", "").strip()
    
    if not val:
        return (0, 0, "")
    
    val_lower = val.lower()

    # 2. é‚è¼¯åˆ¤æ–·
    if "n.d." in val_lower or "nd" == val_lower:
        return (1, 0, "n.d.")
    
    if "negative" in val_lower or "é™°æ€§" in val_lower:
        return (2, 0, "Negative")
    
    # 3. å˜—è©¦æŠ“å–ç´”æ•¸å­— (ç§»é™¤ < æˆ– > ç¬¦è™Ÿä»¥ä¾¿æ¯”å¤§å°)
    # ä¾‹å¦‚ "<5" æˆ‘å€‘ç•¶ä½œ 0 è™•ç†ï¼Œä½†å¦‚æœæœ‰ "100" å‰‡ä¿ç•™
    num_match = re.search(r"([\d\.]+)", val)
    if num_match:
        try:
            number = float(num_match.group(1))
            return (3, number, val) # å›å‚³æ¸…æ´—å¾Œçš„ val (ä¸å«å–®ä½)
        except:
            pass
            
    return (0, 0, val)

# --- 3. æ ¸å¿ƒè§£æé‚è¼¯ ---

def process_files(files):
    data_pool = {key: [] for key in KEYWORD_MAP.keys()}
    all_dates = []
    
    progress_bar = st.progress(0)
    
    for i, file in enumerate(files):
        filename = file.name
        current_date = None
        
        try:
            with pdfplumber.open(file) as pdf:
                # 1. æŠ“å–æ—¥æœŸ
                first_page_text = pdf.pages[0].extract_text()
                current_date = extract_date_from_text(first_page_text)
                if current_date:
                    all_dates.append((current_date, filename))

                # 2. æŠ“å–è¡¨æ ¼
                for page in pdf.pages:
                    tables = page.extract_tables()
                    for table in tables:
                        for row in table:
                            clean_row = [str(cell).replace('\n', ' ').strip() if cell else "" for cell in row]
                            
                            if len(clean_row) >= 5 and "æ¸¬è©¦é …ç›®" not in clean_row[0]:
                                item_name = clean_row[0]
                                unit = clean_row[2]
                                result = clean_row[4] # é è¨­æŠ“ç¬¬5æ¬„
                                
                                # â˜…ä¿®æ­£ï¼šé‡å° DEHP é¡å¯èƒ½æŠ“éŒ¯æ¬„ä½æˆ–æ˜¯å–®ä½é»åœ¨æ•¸å€¼ä¸Šçš„è™•ç†
                                # å¦‚æœç™¼ç¾ result æ¬„ä½æ˜¯ç©ºçš„ï¼Œä½†å¾Œé¢æ¬„ä½æœ‰å€¼ï¼Œå˜—è©¦å¾€å¾ŒæŠ“
                                if not result and len(clean_row) > 5:
                                     # æœ‰æ™‚å€™æ ¼å­æ­ªæ‰ï¼Œè©¦è‘—æŠ“ä¸‹ä¸€æ ¼ï¼Œä½†éœ€å°å¿ƒä¸è¦æŠ“åˆ°é™å€¼
                                     # é€™è£¡æˆ‘å€‘å…ˆç›¸ä¿¡ä¸Šé¢çš„å–®ä½æ¸…æ´—åŠŸèƒ½
                                     pass

                                for target_key, keywords in KEYWORD_MAP.items():
                                    for kw in keywords:
                                        if kw in item_name:
                                            # å¦‚æœæ˜¯ DEHP ç³»åˆ—ï¼Œä¸”æŠ“åˆ°çš„å€¼å¤§æ–¼ 100 (é€šå¸¸çµæœä¸æœƒå‰›å¥½æ˜¯æ•´æ•¸é™å€¼)ï¼Œ
                                            # å¯èƒ½æ˜¯æŠ“åˆ°é™å€¼äº†ã€‚é€™è£¡åšä¸€å€‹ç°¡å–®é˜²å‘†ï¼š
                                            # å¦‚æœ result çœ‹èµ·ä¾†åƒé™å€¼ (å¦‚ "1000") ä¸” row[3] (MDL) å­˜åœ¨ï¼Œ
                                            # æœ‰å¯èƒ½ n.d. å¯«åœ¨ index 3 æˆ– index 5? 
                                            # (æš«ä¸åŠ å…¥éåº¦è¤‡é›œé‚è¼¯ï¼Œå…ˆé  remove unit è§£æ±ºé¡¯ç¤ºå•é¡Œ)
                                            
                                            priority = parse_value_priority(result)
                                            data_pool[target_key].append({
                                                "priority": priority,
                                                "filename": filename,
                                                "date": current_date,
                                                "unit": unit
                                            })
                                            break 
        except Exception as e:
            st.warning(f"æª”æ¡ˆ {filename} è®€å–æ™‚ç™¼ç”Ÿå¾®å°éŒ¯èª¤ï¼Œå·²ç•¥ééƒ¨åˆ†å…§å®¹: {e}")

        progress_bar.progress((i + 1) / len(files))

    # --- 4. æ•¸æ“šèšåˆ ---
    final_row = {}
    max_val_filename = "" 
    global_max_score = -1
    default_unit = ""

    # æ‰¾å‡ºå„é …ç›®çš„æœ€ä½³å€¼
    for key in KEYWORD_MAP.keys():
        candidates = data_pool[key]
        if not candidates:
            final_row[key] = "" 
            continue
            
        # æ’åºï¼šå„ªå…ˆç´š(3>2>1) -> æ•¸å€¼å¤§å° -> 
        best_record = sorted(candidates, key=lambda x: (x['priority'][0], x['priority'][1]), reverse=True)[0]
        final_row[key] = best_record['priority'][2] # å¡«å…¥æ¸…æ´—å¾Œçš„æ–‡å­—
        
        # æŠ“å–®ä½ (å„ªå…ˆæŠ“æœ‰æ•¸å€¼çš„)
        if best_record['priority'][0] == 3 and not default_unit:
            default_unit = best_record['unit']
            
        # æ±ºå®šæœ€å¤§å€¼æª”æ¡ˆ
        if best_record['priority'][0] > global_max_score:
            global_max_score = best_record['priority'][0]
            max_val_filename = best_record['filename']
        elif best_record['priority'][0] == 3 and global_max_score == 3:
             # è‹¥åŒç‚ºæ•¸å€¼ï¼Œé€™è£¡ç°¡å–®æ›´æ–°ç‚ºç•¶å‰æª”æ¡ˆ
             max_val_filename = best_record['filename']

    # æ±ºå®šæ—¥æœŸ
    final_date_str = ""
    latest_file_name_by_date = ""
    if all_dates:
        latest_date_record = sorted(all_dates, key=lambda x: x[0], reverse=True)[0]
        final_date_str = latest_date_record[0].strftime("%d-%b-%Y")
        latest_file_name_by_date = latest_date_record[1]
    
    final_row["å–®ä½"] = default_unit if default_unit else "mg/kg"
    final_row["æ—¥æœŸ"] = final_date_str
    
    # æ±ºå®šæª”æ¡ˆåç¨± (æ•¸å€¼æœ€å¤§è€…å„ªå…ˆï¼Œå¦å‰‡å–æ—¥æœŸæœ€æ–°è€…)
    if global_max_score == 3: 
        final_row["æª”æ¡ˆåç¨±"] = max_val_filename
    else:
        # å¦‚æœå…¨éƒ½æ˜¯ n.d. æˆ– Negativeï¼Œé¡¯ç¤ºæœ€æ–°æ—¥æœŸçš„é‚£å€‹æª”å
        final_row["æª”æ¡ˆåç¨±"] = latest_file_name_by_date if latest_file_name_by_date else (files[0].name if files else "")

    return [final_row]

# --- 5. Streamlit ä»‹é¢ ---
st.set_page_config(page_title="SGS å ±å‘Šèšåˆå·¥å…·", layout="wide")

st.title("ğŸ“„ SGS æª¢æ¸¬å ±å‘Šæ‰¹æ¬¡èšåˆå·¥å…·")
st.info("ğŸ’¡ æç¤ºï¼šè‹¥è¦ä¸Šå‚³å¤šä»½æª”æ¡ˆï¼Œè«‹åœ¨é¸æ“‡è¦–çª—ä¸­æŒ‰ä½ Ctrl æˆ– Shift éµä¸€æ¬¡é¸å–æ‰€æœ‰æª”æ¡ˆã€‚")

uploaded_files = st.file_uploader("è«‹ä¸€æ¬¡é¸å–æ‰€æœ‰ PDF æª”æ¡ˆ", type="pdf", accept_multiple_files=True)

if uploaded_files:
    # é‡æ–°æ•´ç†æŒ‰éˆ• (è§£æ±ºæœ‰æ™‚éœ€è¦é‡è·‘çš„éœ€æ±‚)
    if st.button("ğŸ”„ é‡æ–°åŸ·è¡Œåˆ†æ"):
        st.rerun()

    try:
        result_data = process_files(uploaded_files)
        df = pd.DataFrame(result_data)
        
        # è£œé½Šç©ºæ¬„ä½ä¸¦æ’åº
        for col in OUTPUT_COLUMNS:
            if col not in df.columns:
                df[col] = ""
        df = df[OUTPUT_COLUMNS]

        st.success("âœ… è™•ç†å®Œæˆï¼")
        st.dataframe(df)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Summary')
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel",
            data=output.getvalue(),
            file_name="SGS_Summary.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
    except Exception as e:

        st.error(f"ç™¼ç”ŸéŒ¯èª¤: {e}")
