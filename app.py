import streamlit as st
import pdfplumber
import pandas as pd
import io
import re
from datetime import datetime

# --- 1. å®šç¾©æ¬„ä½èˆ‡é—œéµå­— ---

SIMPLE_KEYWORDS = {
    "Pb": ["Lead", "é‰›", "Pb"],
    "Cd": ["Cadmium", "é˜", "Cd"],
    "Hg": ["Mercury", "æ±", "Hg"],
    "Cr6+": ["Hexavalent Chromium", "å…­åƒ¹é‰»", "Cr(VI)", "Chromium VI"],
    "DEHP": ["DEHP", "Di(2-ethylhexyl) phthalate", "Bis(2-ethylhexyl) phthalate"],
    "BBP": ["BBP", "Butyl benzyl phthalate"],
    "DBP": ["DBP", "Dibutyl phthalate"],
    "DIBP": ["DIBP", "Diisobutyl phthalate"],
    "PFOS": ["PFOS", "Perfluorooctane sulfonates", "Perfluorooctane sulfonate"],
    "F": ["Fluorine", "æ°Ÿ"],
    "CL": ["Chlorine", "æ°¯"],
    "BR": ["Bromine", "æº´"],
    "I": ["Iodine", "ç¢˜"]
}

GROUP_KEYWORDS = {
    "PBB": [
        "Polybrominated Biphenyls (PBBs)", # æ‚¨æŒ‡å®šçš„é—œéµå­—
        "Sum of PBBs", "å¤šæº´è¯è‹¯ç¸½å’Œ",
        "Monobromobiphenyl", "Dibromobiphenyl", "Tribromobiphenyl", 
        "Tetrabromobiphenyl", "Pentabromobiphenyl", "Hexabromobiphenyl", 
        "Heptabromobiphenyl", "Octabromobiphenyl", "Nonabromobiphenyl", 
        "Decabromobiphenyl", "bromobiphenyl"
    ],
    "PBDE": [
        "Polybrominated Diphenyl Ethers (PBDEs)", # æ‚¨æŒ‡å®šçš„é—œéµå­—
        "Sum of PBDEs", "å¤šæº´è¯è‹¯é†šç¸½å’Œ",
        "Monobromodiphenyl ether", "Dibromodiphenyl ether", "Tribromodiphenyl ether",
        "Tetrabromodiphenyl ether", "Pentabromodiphenyl ether", "Hexabromodiphenyl ether",
        "Heptabromodiphenyl ether", "Octabromodiphenyl ether", "Nonabromodiphenyl ether",
        "Decabromodiphenyl ether", "bromodiphenyl ether"
    ],
    "PFAS": [
        "PFHxA", "PFOA", "PFNA", "PFDA", "PFUnDA", "PFDoDA", "PFTrDA", "PFTeDA",
        "FTOH", "FTA", "FTMAC", "FTS", "FTCA", "PFAS", "Perfluoro", "å…¨æ°Ÿ"
    ]
}

PFAS_TRIGGER_PHRASES = [
    "Per- and Polyfluoroalkyl Substances",
    "PFHxA and its salts",
    "å…¨æ°Ÿ/å¤šæ°Ÿçƒ·åŸºç‰©è³ª"
]

OUTPUT_COLUMNS = [
    "Pb", "Cd", "Hg", "Cr6+", "PBB", "PBDE", 
    "DEHP", "BBP", "DBP", "DIBP", 
    "PFOS", "PFAS", "F", "CL", "BR", "I", 
    "æ—¥æœŸ", "æª”æ¡ˆåç¨±"
]

# --- 2. è¼”åŠ©åŠŸèƒ½ ---

def clean_text(text):
    if not text: return ""
    return str(text).replace('\n', ' ').strip()

def extract_date_from_text(text):
    text = clean_text(text)
    # å¼·åŠ›æ—¥æœŸåŒ¹é…ï¼šåŒ…å« Date: 2025/01/01 æˆ– Issue Date: 06-Jan-2025
    patterns = [
        r"(?:Date|æ—¥æœŸ|Issue).*?([0-9]{4})[/\.-]([0-9]{1,2})[/\.-]([0-9]{1,2})", # 2025/01/06
        r"(?:Date|æ—¥æœŸ|Issue).*?([0-9]{2}-[a-zA-Z]{3}-[0-9]{4})", # 06-Jan-2025
        r"([0-9]{4})[/\.-]([0-9]{1,2})[/\.-]([0-9]{1,2})" # ç´”æ—¥æœŸæ ¼å¼ (å‚™æ´)
    ]
    
    for pattern in patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            try:
                groups = match.groups()
                if len(groups) == 3: # YYYY/MM/DD
                    return datetime(int(groups[0]), int(groups[1]), int(groups[2]))
                elif len(groups) == 1: # DD-Mon-YYYY
                    return datetime.strptime(groups[0], "%d-%b-%Y")
            except: continue
    return None

def parse_value_priority(value_str):
    """
    æ•¸å€¼è§£æé‚è¼¯ï¼š
    å›å‚³ (åˆ†æ•¸, æ•¸å€¼, é¡¯ç¤ºæ–‡å­—)
    3: æœ‰æ•¸å€¼ (10.5)
    2: Negative
    1: N.D.
    0: ç„¡æ•ˆ
    """
    # ç§»é™¤å–®ä½èˆ‡é›œè¨Š
    val = clean_text(value_str).replace("mg/kg", "").replace("ppm", "").replace("%", "").replace("Âµg/cmÂ²", "").strip()
    
    if not val: return (0, 0, "")
    val_lower = val.lower()

    # æ’é™¤ä¸æ˜¯çµæœçš„å­—
    if val_lower in ["result", "limit", "mdl", "loq", "unit", "method", "004", "no.1", "---", "-"]: 
        return (0, 0, "")

    if "n.d." in val_lower or "nd" == val_lower or "<" in val_lower: 
        return (1, 0, "n.d.")
    if "negative" in val_lower or "é™°æ€§" in val_lower: 
        return (2, 0, "Negative")
    
    # æŠ“å–ç´”æ•¸å­— (åŒ…å«å°æ•¸é»)
    num_match = re.search(r"([\d\.]+)", val)
    if num_match:
        try:
            number = float(num_match.group(1))
            return (3, number, val)
        except: pass
            
    return (0, 0, val)

# --- 3. æ ¸å¿ƒï¼šå‹•æ…‹æ¬„ä½è­˜åˆ¥ ---

def check_pfas_trigger(full_text):
    for phrase in PFAS_TRIGGER_PHRASES:
        if phrase.lower() in full_text.lower():
            return True
    return False

def identify_columns(header_row):
    """
    æ™ºæ…§åˆ¤æ–· Result åœ¨å“ªä¸€æ¬„
    """
    item_idx = -1
    result_idx = -1
    
    for i, cell in enumerate(header_row):
        txt = clean_text(cell).lower()
        if "test item" in txt or "tested item" in txt or "æ¸¬è©¦é …ç›®" in txt: item_idx = i
        if "result" in txt or "çµæœ" in txt: result_idx = i
            
    return item_idx, result_idx

def process_files(files):
    data_pool = {key: [] for key in OUTPUT_COLUMNS if key not in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]}
    all_dates = []
    
    # Pb æœ€å¤§å€¼è¿½è¹¤å™¨
    pb_tracker = {
        "max_score": -1, # 0=ç„¡, 1=nd, 2=neg, 3=num
        "max_value": -1.0,
        "filename": ""
    }
    
    progress_bar = st.progress(0)
    
    for i, file in enumerate(files):
        filename = file.name
        
        file_group_data = {key: [] for key in GROUP_KEYWORDS.keys()}
        full_text_content = ""

        try:
            with pdfplumber.open(file) as pdf:
                # 1. æŠ“æ—¥æœŸ (æƒæå‰ä¸‰é ï¼Œç¯„åœæ“´å¤§)
                date_found = None
                for p_idx in range(min(3, len(pdf.pages))):
                    page_txt = pdf.pages[p_idx].extract_text()
                    if page_txt:
                        full_text_content += page_txt
                        if not date_found:
                            d = extract_date_from_text(page_txt)
                            if d: date_found = d
                            
                if date_found:
                    all_dates.append((date_found, filename))
                
                # è£œè®€å‰©é¤˜é é¢
                for p in pdf.pages[3:]:
                    full_text_content += (p.extract_text() or "")

                pfas_active = check_pfas_trigger(full_text_content)

                # 2. æŠ“è¡¨æ ¼
                last_result_idx = -1 
                last_item_idx = 0

                for page in pdf.pages:
                    tables = page.extract_tables()
                    for table in tables:
                        if not table or len(table) < 2: continue
                        
                        header_row = table[0]
                        item_idx, result_idx = identify_columns(header_row)
                        
                        # è¡¨é ­è¨˜æ†¶ï¼šå¦‚æœç•¶å‰è¡¨æ ¼æ²’è¡¨é ­ï¼Œæ²¿ç”¨ä¸Šä¸€å€‹
                        if result_idx != -1:
                            last_result_idx = result_idx
                            last_item_idx = item_idx if item_idx != -1 else 0
                        else:
                            if last_result_idx != -1:
                                result_idx = last_result_idx
                                item_idx = last_item_idx
                        
                        for row_idx, row in enumerate(table):
                            clean_row = [clean_text(cell) for cell in row]
                            # è·³éé¡¯ç„¶æ˜¯è¡¨é ­çš„è¡Œ
                            row_text_joined = "".join(clean_row).lower()
                            if "test item" in row_text_joined or "result" in row_text_joined: continue
                            if not any(clean_row): continue
                            
                            # æ‰¾æ¸¬é …
                            target_item_col = item_idx if item_idx != -1 else 0
                            if target_item_col >= len(clean_row): continue
                            item_name = clean_row[target_item_col]
                            
                            # æ‰¾çµæœ
                            result = ""
                            # å„ªå…ˆï¼šä¾æ¬„ä½ç´¢å¼•
                            if result_idx != -1 and result_idx < len(clean_row):
                                result = clean_row[result_idx]
                            
                            # å‚™æ´ï¼šç‰¹å¾µæœå°‹ (æ‰¾ nd æˆ– æ•¸å­—)
                            # é‡å° "Tin Layer" é€™ç¨®æ ¼å¼ï¼Œæœ‰æ™‚å€™ Result åœ¨ Unit çš„å¾Œé¢
                            if not result:
                                for cell in reversed(clean_row):
                                    c_lower = cell.lower()
                                    if not cell: continue
                                    if "n.d." in c_lower or "negative" in c_lower or re.search(r"^\d+(\.\d+)?$", cell):
                                        # ç°¡å–®éæ¿¾ï¼šå¦‚æœé€™æ ¼é•·å¾—åƒ MDL (æ•´æ•¸ 2, 5, 10)ï¼Œä¸”å‰é¢é‚„æœ‰ä¸€æ ¼ä¹Ÿæ˜¯æ•¸å­—ï¼Œå¯èƒ½æŠ“éŒ¯
                                        # ä½†é€™è£¡å…ˆç›¸ä¿¡å®ƒ
                                        result = cell
                                        break
                            
                            priority = parse_value_priority(result)
                            if priority[0] == 0: continue 

                            # --- A. Simple (Pb/Cd...) ---
                            for target_key, keywords in SIMPLE_KEYWORDS.items():
                                for kw in keywords:
                                    if kw.lower() in item_name.lower():
                                        if target_key == "PFOS" and "related" in item_name.lower(): continue 
                                        
                                        data_pool[target_key].append({
                                            "priority": priority,
                                            "filename": filename
                                        })
                                        
                                        # â˜… Pb æœ€å¤§å€¼æª”æ¡ˆè¿½è¹¤ â˜…
                                        if target_key == "Pb":
                                            # é‚è¼¯ï¼šæœ‰æ•¸å€¼(3) > Negative(2) > n.d.(1)
                                            # å¦‚æœæ‰¾åˆ°æ›´å¤§çš„åˆ†æ•¸ï¼Œæˆ–è€…åŒåˆ†ä½†æ•¸å€¼æ›´å¤§ï¼Œå°±æ›´æ–°
                                            if priority[0] > pb_tracker["max_score"]:
                                                pb_tracker["max_score"] = priority[0]
                                                pb_tracker["max_value"] = priority[1]
                                                pb_tracker["filename"] = filename
                                            elif priority[0] == 3 and priority[1] > pb_tracker["max_value"]:
                                                pb_tracker["max_value"] = priority[1]
                                                pb_tracker["filename"] = filename
                                        break

                            # --- B. Group (PBB/PBDE/PFAS) ---
                            for group_key, keywords in GROUP_KEYWORDS.items():
                                if group_key == "PFAS" and not pfas_active: continue

                                for kw in keywords:
                                    if kw.lower() in item_name.lower():
                                        if group_key == "PFAS" and "pfos" in item_name.lower() and "related" not in item_name.lower():
                                            continue
                                        
                                        # ä¸ç®¡æ˜¯ä¸æ˜¯ "Sum of"ï¼Œåªè¦æŠ“åˆ°å°±ç´å…¥è¨ˆç®—
                                        file_group_data[group_key].append(priority)
                                        break
            
            # --- æª”æ¡ˆçµç®— (PBB/PBDE/PFAS) ---
            for group_key, values in file_group_data.items():
                if values:
                    # é‚è¼¯ï¼šä¸€ä»½å ±å‘Šä¸­ï¼Œåªè¦æœ‰ä¸€å€‹ç´°é …æ˜¯æ•¸å€¼ï¼Œå°±å–æœ€å¤§å€¼ã€‚å…¨éƒ½æ˜¯ n.d. æ‰æ˜¯ n.d.
                    best_in_file = sorted(values, key=lambda x: (x[0], x[1]), reverse=True)[0]
                    data_pool[group_key].append({
                        "priority": best_in_file,
                        "filename": filename
                    })

        except Exception as e:
            st.warning(f"æª”æ¡ˆ {filename} è§£æç•°å¸¸: {e}")

        progress_bar.progress((i + 1) / len(files))

    # --- 4. èšåˆ ---
    final_row = {}

    for key in OUTPUT_COLUMNS:
        if key in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]: continue
        
        candidates = data_pool.get(key, [])
        if not candidates:
            final_row[key] = "" 
            continue
            
        # å–æ‰€æœ‰å ±å‘Šä¸­æœ€å¤§çš„é‚£å€‹å€¼
        best_record = sorted(candidates, key=lambda x: (x['priority'][0], x['priority'][1]), reverse=True)[0]
        final_row[key] = best_record['priority'][2]

    # æ—¥æœŸè™•ç† (å–æœ€æ–°)
    final_date_str = ""
    latest_file = ""
    if all_dates:
        latest_date_record = sorted(all_dates, key=lambda x: x[0], reverse=True)[0]
        final_date_str = latest_date_record[0].strftime("%Y/%m/%d")
        latest_file = latest_date_record[1] # å‚™ç”¨ï¼šæ—¥æœŸæœ€æ–°çš„æª”æ¡ˆ
    
    final_row["æ—¥æœŸ"] = final_date_str
    
    # â˜… æª”æ¡ˆåç¨±é‚è¼¯ï¼šé¡¯ç¤º Pb å€¼æœ€å¤§çš„æª”æ¡ˆ â˜…
    if pb_tracker["filename"]:
        final_row["æª”æ¡ˆåç¨±"] = pb_tracker["filename"]
    else:
        # å¦‚æœ Pb å…¨éƒ½æ²’æŠ“åˆ°ï¼Œæ”¹é¡¯ç¤ºæ—¥æœŸæœ€æ–°çš„æª”æ¡ˆ (é˜²å‘†)
        final_row["æª”æ¡ˆåç¨±"] = latest_file if latest_file else (files[0].name if files else "")

    return [final_row]

# --- ä»‹é¢ ---
st.set_page_config(page_title="SGS å ±å‘Šèšåˆå·¥å…· v11.0", layout="wide")
st.title("ğŸ“„ è¬ç”¨å‹æª¢æ¸¬å ±å‘Šèšåˆå·¥å…· (v11.0)")
st.info("ğŸ’¡ æ›´æ–°ï¼šä¿®æ­£ Pb æœ€å¤§å€¼æª”æ¡ˆè¿½è¹¤ã€æ—¥æœŸæ ¼å¼æ”¯æ´ã€PBB/PBDE ç¾¤çµ„é—œéµå­—ã€‚")

uploaded_files = st.file_uploader("è«‹ä¸€æ¬¡é¸å–æ‰€æœ‰ PDF æª”æ¡ˆ", type="pdf", accept_multiple_files=True)

if uploaded_files:
    if st.button("ğŸ”„ é‡æ–°åŸ·è¡Œ"): st.rerun()

    try:
        result_data = process_files(uploaded_files)
        df = pd.DataFrame(result_data)
        
        for col in OUTPUT_COLUMNS:
            if col not in df.columns: df[col] = ""
        df = df[OUTPUT_COLUMNS]

        st.success("âœ… è™•ç†å®Œæˆï¼")
        st.dataframe(df)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Summary')
        
        st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel", data=output.getvalue(), file_name="SGS_Summary_v11.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        
    except Exception as e:
        st.error(f"ç³»çµ±éŒ¯èª¤: {e}")
