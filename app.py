import streamlit as st
import pdfplumber
import pandas as pd
import io
import re
from datetime import datetime

# --- 1. å®šç¾©æ¬„ä½èˆ‡é—œéµå­— ---

# é€™äº›æ˜¯éœ€è¦ "æŠ“æ•¸å€¼" çš„é …ç›® (RoHS å‚³çµ±é …ç›®)
# ç¨‹å¼æœƒå»è¡¨æ ¼è£¡æ‰¾çµæœ
SIMPLE_KEYWORDS = {
    "Pb": ["Lead", "é‰›", "Pb"],
    "Cd": ["Cadmium", "é˜", "Cd"],
    "Hg": ["Mercury", "æ±", "Hg"],
    "Cr6+": ["Hexavalent Chromium", "å…­åƒ¹é‰»", "Cr(VI)", "Chromium VI"],
    "DEHP": ["DEHP", "Di(2-ethylhexyl) phthalate", "Bis(2-ethylhexyl) phthalate"],
    "BBP": ["BBP", "Butyl benzyl phthalate"],
    "DBP": ["DBP", "Dibutyl phthalate"],
    "DIBP": ["DIBP", "Diisobutyl phthalate"],
    "PFOS": ["PFOS", "Perfluorooctane sulfonates", "Perfluorooctane sulfonate"], 
    "F": ["Fluorine", "æ°Ÿ"],
    "CL": ["Chlorine", "æ°¯"],
    "BR": ["Bromine", "æº´"],
    "I": ["Iodine", "ç¢˜"]
}

# é€™äº›æ˜¯éœ€è¦ "æŠ“ç¾¤çµ„æœ€å¤§å€¼" çš„é …ç›®
GROUP_KEYWORDS = {
    "PBB": [
        "Polybrominated Biphenyls", "PBBs", "Sum of PBBs", "å¤šæº´è¯è‹¯ç¸½å’Œ",
        "Monobromobiphenyl", "Dibromobiphenyl", "Tribromobiphenyl", 
        "Tetrabromobiphenyl", "Pentabromobiphenyl", "Hexabromobiphenyl", 
        "Heptabromobiphenyl", "Octabromobiphenyl", "Nonabromobiphenyl", 
        "Decabromobiphenyl", "bromobiphenyl"
    ],
    "PBDE": [
        "Polybrominated Diphenyl Ethers", "PBDEs", "Sum of PBDEs", "å¤šæº´è¯è‹¯é†šç¸½å’Œ",
        "Monobromodiphenyl ether", "Dibromodiphenyl ether", "Tribromodiphenyl ether",
        "Tetrabromodiphenyl ether", "Pentabromodiphenyl ether", "Hexabromodiphenyl ether",
        "Heptabromodiphenyl ether", "Octabromodiphenyl ether", "Nonabromodiphenyl ether",
        "Decabromodiphenyl ether", "bromodiphenyl ether"
    ]
}

# â˜… PFAS é—œéµå­— (åªç”¨æ–¼æª¢æŸ¥å‰å…©é æ‘˜è¦ï¼Œåˆ¤å®šæ˜¯å¦é¡¯ç¤º REPORT) â˜…
PFAS_SUMMARY_KEYWORDS = [
    "Per- and Polyfluoroalkyl Substances",
    "PFAS",
    "å…¨æ°Ÿ/å¤šæ°Ÿçƒ·åŸºç‰©è³ª",
    "Perfluorooctanoic acid (PFOA) and its salts", 
    "å…¨æ°ŸåŒ–åˆç‰©",
    "Perfluoro"
]

OUTPUT_COLUMNS = [
    "Pb", "Cd", "Hg", "Cr6+", "PBB", "PBDE", 
    "DEHP", "BBP", "DBP", "DIBP", 
    "PFOS", "PFAS", "F", "CL", "BR", "I", 
    "æ—¥æœŸ", "æª”æ¡ˆåç¨±"
]

# --- 2. è¼”åŠ©åŠŸèƒ½ ---

def clean_text(text):
    if not text: return ""
    return str(text).replace('\n', ' ').strip()

def extract_date_from_text(text):
    """
    v20.0: è¬èƒ½æ—¥æœŸæŠ“å– (æ”¯æ´ SGS, Intertek, CTI å„ç¨®æ ¼å¼)
    """
    text = clean_text(text)
    
    # å®šç¾©å„ç¨®å¯èƒ½çš„æ—¥æœŸæ ¼å¼ Regex
    patterns = [
        # æ ¼å¼ A: 2025/01/06, 2025-01-06 (YYYYé–‹é ­)
        r"(20\d{2})[/\.-](0?[1-9]|1[0-2])[/\.-](0?[1-9]|[12][0-9]|3[01])",
        
        # æ ¼å¼ B: 06-Jan-2025 (DD-Mon-YYYY) - å…è¨±æ©«ç·šå‘¨åœæœ‰ç©ºæ ¼
        r"(0?[1-9]|[12][0-9]|3[01])\s*[-/]\s*([a-zA-Z]{3})\s*[-/]\s*(20\d{2})",
        
        # æ ¼å¼ C: Dec. 26, 2024 æˆ– Oct 08 2024 (Mon DD YYYY) - å…è¨±é»ã€é€—è™Ÿã€ç©ºæ ¼
        r"([a-zA-Z]{3})\.?\s+(0?[1-9]|[12][0-9]|3[01])[,\s]+\s*(20\d{2})"
    ]
    
    found_dates = []
    for pattern in patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            try:
                dt = None
                full_match = match.group(0)
                
                # ç°¡å–®åŒ–è™•ç†ï¼šæŠŠæ‰€æœ‰ç¬¦è™Ÿ (.,-) éƒ½æ›æˆç©ºç™½ï¼Œç„¶å¾Œå˜—è©¦è§£æ
                # é€™æ¨£ Dec. 26, 2024 -> Dec 26 2024
                clean_str = full_match.replace(".", " ").replace(",", " ").replace("-", " ").replace("/", " ")
                # ç¸®æ¸›å¤šé¤˜ç©ºç™½
                clean_str = " ".join(clean_str.split())
                
                # å˜—è©¦å¸¸è¦‹çš„æ™‚é–“æ ¼å¼å­—ä¸²
                for fmt in ["%Y %m %d", "%d %b %Y", "%b %d %Y"]:
                    try:
                        dt = datetime.strptime(clean_str, fmt)
                        break
                    except: continue
                
                # å¹´ä»½éæ¿¾ (2000~2030)ï¼Œé¿å…æŠ“åˆ°æ³•è¦ç·¨è™Ÿ (å¦‚ 2321)
                if dt and 2000 <= dt.year <= 2030: 
                    found_dates.append(dt)
            except: continue
            
    if found_dates:
        return max(found_dates) # å›å‚³æœ€æ–°æ—¥æœŸ
    return None

def is_suspicious_limit_value(val):
    """æ•¸å€¼é˜²ç«ç‰†ï¼šå¦‚æœæ˜¯é€™äº›å¸¸è¦‹é™å€¼ï¼Œè¦–ç‚ºé«˜é¢¨éšªï¼Œä¸æŠ“å–"""
    try:
        n = float(val)
        if n in [1000.0, 100.0, 50.0]: return True
        return False
    except: return False

def parse_value_priority(value_str):
    raw_val = clean_text(value_str)
    # è™•ç† "0.01 (100)" é€™ç¨®æ‹¬è™Ÿæ ¼å¼
    if "(" in raw_val: raw_val = raw_val.split("(")[0].strip()
    
    val = raw_val.replace("mg/kg", "").replace("ppm", "").replace("%", "").replace("Âµg/cmÂ²", "").strip()
    
    if not val: return (0, 0, "")
    val_lower = val.lower()

    # é»‘åå–®éæ¿¾
    if val_lower in ["result", "limit", "mdl", "loq", "rl", "unit", "method", "004", "001", "no.1", "---", "-", "limits"]: 
        return (0, 0, "")

    # æ•¸å€¼é˜²ç«ç‰†
    if is_suspicious_limit_value(val): return (0, 0, "") 

    if "nd" in val_lower or "n.d." in val_lower or "<" in val_lower: 
        return (1, 0, "n.d.")
    if "negative" in val_lower or "é™°æ€§" in val_lower: 
        return (2, 0, "Negative")
    
    num_match = re.search(r"([\d\.]+)", val)
    if num_match:
        try:
            number = float(num_match.group(1))
            return (3, number, num_match.group(1))
        except: pass
            
    return (0, 0, val)

# --- 3. æ ¸å¿ƒé‚è¼¯ ---

def check_pfas_in_summary(text):
    """æª¢æŸ¥æ–‡å­—ä¸­æ˜¯å¦æœ‰ PFAS ç›¸é—œé—œéµå­—"""
    txt_lower = text.lower()
    for kw in PFAS_SUMMARY_KEYWORDS:
        if kw.lower() in txt_lower:
            return True
    return False

def identify_columns(table):
    item_idx = -1
    result_idx = -1
    
    max_scan_rows = min(3, len(table))
    full_header_text = ""
    for r in range(max_scan_rows):
        full_header_text += " ".join([str(c).lower() for c in table[r] if c]) + " "
    
    # é™å€¼è¡¨/åƒè€ƒè¡¨éæ¿¾ (åªè¦æ¨™é¡Œçœ‹èµ·ä¾†åƒæ¸…å–®ä¸”æ²’çµæœï¼Œå°±è·³é)
    if ("restricted substances" in full_header_text or "limits" in full_header_text or "substance name" in full_header_text) and \
       not any(x in full_header_text for x in ["result", "çµæœ", "00", "no.", "green"]):
        return -1, -1, True 

    for r_idx in range(max_scan_rows):
        row = table[r_idx]
        for c_idx, cell in enumerate(row):
            txt = clean_text(cell).lower()
            if not txt: continue
            
            if "test item" in txt or "tested item" in txt or "æ¸¬è©¦é …ç›®" in txt:
                if item_idx == -1: item_idx = c_idx
            
            # çµæœæ¬„ä½è­˜åˆ¥ (æ“´å……é—œéµå­—)
            if ("result" in txt or "çµæœ" in txt or re.search(r"00[1-9]", txt) or 
                "no." in txt or "green" in txt or "submitted" in txt or "composite" in txt):
                if result_idx == -1: result_idx = c_idx

    return item_idx, result_idx, False

def process_files(files):
    data_pool = {key: [] for key in OUTPUT_COLUMNS if key not in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]}
    all_dates = []
    pb_tracker = {"max_score": -1, "max_value": -1.0, "filenames": []}
    
    progress_bar = st.progress(0)
    
    for i, file in enumerate(files):
        filename = file.name
        file_group_data = {key: [] for key in GROUP_KEYWORDS.keys()}
        
        try:
            with pdfplumber.open(file) as pdf:
                # 1. æŠ“æ—¥æœŸ & æª¢æŸ¥ PFAS éœ€æ±‚ (æƒæå‰2é )
                file_dates = []
                first_few_pages_text = ""
                
                for p_idx in range(min(2, len(pdf.pages))):
                    page_txt = pdf.pages[p_idx].extract_text()
                    if page_txt:
                        first_few_pages_text += page_txt
                        d = extract_date_from_text(page_txt)
                        if d: file_dates.append(d)
                
                if file_dates: all_dates.append((max(file_dates), filename))
                
                # â˜… PFAS åˆ¤å®šï¼šç›´æ¥çœ‹å‰å…©é æœ‰æ²’æœ‰å¯«è¦æ¸¬ PFAS â˜…
                if check_pfas_in_summary(first_few_pages_text):
                    # 4åˆ†ä»£è¡¨æ¥µé«˜å„ªå…ˆç´šï¼Œç¢ºä¿è“‹éä»»ä½•èª¤æŠ“çš„é›œè¨Š
                    data_pool["PFAS"].append({
                        "priority": (4, 0, "REPORT"), 
                        "filename": filename
                    })

                # 2. æŠ“è¡¨æ ¼ (åƒ…é‡å° RoHS é …ç›®)
                last_result_idx = -1 
                last_item_idx = 0

                for page in pdf.pages:
                    tables = page.extract_tables()
                    for table in tables:
                        if not table or len(table) < 2: continue
                        
                        item_idx, result_idx, is_skip_table = identify_columns(table)
                        
                        if is_skip_table: continue 

                        if result_idx != -1:
                            last_result_idx = result_idx
                            last_item_idx = item_idx if item_idx != -1 else 0
                        else:
                            if last_result_idx != -1 and len(table[0]) > 3:
                                result_idx = last_result_idx
                                item_idx = last_item_idx
                        
                        for row_idx, row in enumerate(table):
                            clean_row = [clean_text(cell) for cell in row]
                            row_txt = "".join(clean_row).lower()
                            if "test item" in row_txt or "result" in row_txt or "restricted" in row_txt: continue
                            if not any(clean_row): continue
                            
                            target_item_col = item_idx if item_idx != -1 else 0
                            if target_item_col >= len(clean_row): continue
                            item_name = clean_row[target_item_col]
                            
                            # PVC æ’é™¤ (é¿å…æŠ“åˆ° Cl çš„ Negative)
                            if "pvc" in item_name.lower() or "polyvinyl" in item_name.lower(): continue

                            result = ""
                            if result_idx != -1 and result_idx < len(clean_row):
                                result = clean_row[result_idx]
                            
                            if not result:
                                for cell in reversed(clean_row):
                                    c_lower = cell.lower()
                                    if not cell: continue
                                    if "nd" in c_lower or "n.d." in c_lower or "negative" in c_lower:
                                        result = cell
                                        break
                                    if re.search(r"^\d+(\.\d+)?$", cell):
                                        # ç°¡å–®éæ¿¾é™å€¼
                                        if float(cell) in [1000, 100, 50]: continue
                                        result = cell
                                        break
                            
                            priority = parse_value_priority(result)
                            if priority[0] == 0: continue 

                            # Simple Keywords (Pb, Cd, PFOS...)
                            for target_key, keywords in SIMPLE_KEYWORDS.items():
                                for kw in keywords:
                                    if kw.lower() in item_name.lower():
                                        if target_key == "PFOS" and "related" in item_name.lower(): continue 
                                        
                                        data_pool[target_key].append({
                                            "priority": priority,
                                            "filename": filename
                                        })
                                        
                                        if target_key == "Pb":
                                            current_score = priority[0]
                                            current_val = priority[1]
                                            if current_score > pb_tracker["max_score"]:
                                                pb_tracker["max_score"] = current_score
                                                pb_tracker["max_value"] = current_val
                                                pb_tracker["filenames"] = [filename]
                                            elif current_score == 3 and current_val > pb_tracker["max_value"]:
                                                pb_tracker["max_value"] = current_val
                                                pb_tracker["filenames"] = [filename]
                                            elif current_score == 3 and current_val == pb_tracker["max_value"]:
                                                if filename not in pb_tracker["filenames"]:
                                                    pb_tracker["filenames"].append(filename)
                                        break

                            # Group Keywords (PBB/PBDE)
                            for group_key, keywords in GROUP_KEYWORDS.items():
                                for kw in keywords:
                                    if kw.lower() in item_name.lower():
                                        file_group_data[group_key].append(priority)
                                        break
            
            # æª”æ¡ˆçµç®— (PBB/PBDE)
            for group_key, values in file_group_data.items():
                if values:
                    best_in_file = sorted(values, key=lambda x: (x[0], x[1]), reverse=True)[0]
                    data_pool[group_key].append({
                        "priority": best_in_file,
                        "filename": filename
                    })

        except Exception as e:
            st.warning(f"æª”æ¡ˆ {filename} è§£æç•°å¸¸: {e}")
        
        progress_bar.progress((i + 1) / len(files))

    # èšåˆ
    final_row = {}
    for key in OUTPUT_COLUMNS:
        if key in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]: continue
        candidates = data_pool.get(key, [])
        if not candidates:
            final_row[key] = "" 
            continue
        best_record = sorted(candidates, key=lambda x: (x['priority'][0], x['priority'][1]), reverse=True)[0]
        final_row[key] = best_record['priority'][2]

    # æ—¥æœŸèˆ‡æª”å
    final_date_str = ""
    latest_file = ""
    if all_dates:
        latest_date_record = sorted(all_dates, key=lambda x: x[0], reverse=True)[0]
        final_date_str = latest_date_record[0].strftime("%Y/%m/%d")
        latest_file = latest_date_record[1]
    
    final_row["æ—¥æœŸ"] = final_date_str
    
    if pb_tracker["filenames"]:
        final_row["æª”æ¡ˆåç¨±"] = ", ".join(pb_tracker["filenames"])
    else:
        final_row["æª”æ¡ˆåç¨±"] = latest_file if latest_file else (files[0].name if files else "")

    return [final_row]

# --- ä»‹é¢ ---
st.set_page_config(page_title="SGS å ±å‘Šèšåˆå·¥å…· v20.0", layout="wide")
st.title("ğŸ“„ è¬ç”¨å‹æª¢æ¸¬å ±å‘Šèšåˆå·¥å…· (v20.0 æœ€çµ‚ç‰ˆ)")
st.info("ğŸ’¡ v20.0 æ›´æ–°ï¼šæ—¥æœŸè¬èƒ½æ”¯æ´ã€PFAS æ”¹ç‚ºåµæ¸¬éœ€æ±‚(é¡¯ç¤º REPORT)ã€ç¶­æŒé«˜ç²¾åº¦æ•¸å€¼æŠ“å–ã€‚")

uploaded_files = st.file_uploader("è«‹ä¸€æ¬¡é¸å–æ‰€æœ‰ PDF æª”æ¡ˆ", type="pdf", accept_multiple_files=True)

if uploaded_files:
    if st.button("ğŸ”„ é‡æ–°åŸ·è¡Œ"): st.rerun()

    try:
        result_data = process_files(uploaded_files)
        df = pd.DataFrame(result_data)
        for col in OUTPUT_COLUMNS:
            if col not in df.columns: df[col] = ""
        df = df[OUTPUT_COLUMNS]

        st.success("âœ… è™•ç†å®Œæˆï¼")
        st.dataframe(df)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Summary')
        
        st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel", data=output.getvalue(), file_name="SGS_Summary_v20.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        
    except Exception as e:
        st.error(f"ç³»çµ±éŒ¯èª¤: {e}")
